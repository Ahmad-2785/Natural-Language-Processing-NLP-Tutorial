## Index
![deep](https://user-images.githubusercontent.com/12748752/150695343-8977b5d0-3cd4-4959-b90e-9fe72d336d42.png)
![light](https://user-images.githubusercontent.com/12748752/150695340-c086876c-1e29-4493-b03b-cbff51dba02a.png)

## Word Embeddings
![deep](https://user-images.githubusercontent.com/12748752/150695343-8977b5d0-3cd4-4959-b90e-9fe72d336d42.png)
* **Word Embeddings or Word vectorization is a methodology in NLP to map _words_ or _phrases from vocabulary_ to a corresponding vector of real numbers which used to find word predictions, word similarities/semantics.**

### What is Word Embedding?
* Word embedding is just a fancy name for a [_`feature vector`_](https://ds055uzetaobb.cloudfront.net/brioche/uploads/JERsKXkW4T-screen-shot-2016-05-05-at-123118-pm.png) that represents a word.
* We can take a categorical object (a word in this case) and then map this object to a list of numbers in other words a vector we say we have embedded this word into a vector space. 
* So that's why we call them word embeddings.
### Word Embedding properties
* The operation (text to number) or vectorization is done either on **"word"** or on "**character**" level.
* The process of converting words into numbers are called **Vectorization**.
* This is one of the most important advances in Deep NLP research.
* Word Emeddings allow you to map words into a vector space.
* **Once you can represent something as a vector, you can perform arithmetic on it.** So, this is where the famous **_king - man = queen - woman_**, **_December - Novemeber = July - June_** or **_France - Paris = England - London_** come from.
* There are two most popular algorithms for finding Word Embeddings-
    - **Word2vac**
    - **GloVe**

