## Index
![deep](https://user-images.githubusercontent.com/12748752/150695343-8977b5d0-3cd4-4959-b90e-9fe72d336d42.png)

## Word Embeddings
![deep](https://user-images.githubusercontent.com/12748752/150695343-8977b5d0-3cd4-4959-b90e-9fe72d336d42.png)
* **Word Embeddings or Word vectorization is a methodology in NLP to map _words_ or _phrases from vocabulary_ to a corresponding vector of real numbers which used to find word predictions, word similarities/semantics.**

### Word Embedding properties
* The operation (text to number) or vectorization is done either on **"word"** or on "**character**" level.
* The process of converting words into numbers are called **Vectorization**.
* This is one of the most important advances in Deep NLP research.
* Word Emeddings allow you to map words into a vector space.
* Once you can represent something as a vector, you can perform arithmetic on it.
* So this is where the famous **king - man = queen - woman**, **December - Novemeber = July - June** or **France - Paris = England - London** come from.
* There are two most popular algorithms for finding Word Embeddings-
    - **Word2vac**
    - **GloVe**
### What is Word Embedding?
![light](https://user-images.githubusercontent.com/12748752/150695340-c086876c-1e29-4493-b03b-cbff51dba02a.png)

* Word embedding is just a fancy name for a feature vector that represents a word.
* We can take a categorical object a word in this case and then map this object to a list of numbers in other words a vector we say we have embedded this word into a vector space. So that's why we call them word embeddings.

![featurevectors](https://ds055uzetaobb.cloudfront.net/brioche/uploads/JERsKXkW4T-screen-shot-2016-05-05-at-123118-pm.png?width=800)

